Abstract:A daily systemic risk measure is proposed accounting for links and mutualdependencies between financial institutions utilising tail event information. FRM(Financial Risk Meter) is based on Lasso quantile regression designed to capturetail event co-movements. The FRM focus lies on understanding active set datacharacteristics and the presentation of interdependencies in a network topology.Two FRM indices are presented, namely, FRM@Americas and FRM@Europe. The FRMindices detect systemic risk at selected areas and identifies risk factors. Inpractice, FRM is applied to the return time series of selected financialinstitutions and macroeconomic risk factors. Using FRM on a daily basis, weidentify companies exhibiting extreme "co-stress", as well as "activators" ofstress. With the SRM@EuroArea, we extend to the government bond asset class. FRMis a good predictor for recession probabilities, constituting the FRM-impliedrecession probabilities. Thereby, FRM indicates tail event behaviour in anetwork of financial risk factors.Keywords:Systemic Risk, Quantile Regression, Financial Markets, Risk Management, NetworkDynamics, Recession
SEP
Abstract:This research analyses high-frequency data of the cryptocurrency market inregards to intraday trading patterns. We study trading quantitatives such asreturns, traded volumes, volatility periodicity, and provide summary statisticsof return correlations to CRIX (CRyptocurrency IndeX), as well as respectiveoverall high-frequency based market statistics. Our results provide mandatoryinsight into a market, where the grand scale employment of automated tradingalgorithms and the extremely rapid execution of trades might seem to be astandard based on media reports. Our findings on intraday momentum of tradingpatterns lead to a new view on approaching the predictability of economic valuein this new digital market.Keywords:Cryptocurrency, High-Frequency Trading, Algorithmic Trading, Liquidity,Volatility, Price Impact, CRIX
SEP
Abstract:We propose an approach to calibrate the conditional value-at-risk (CoVaR) offinancial institutions based on neural network quantile regression. Building onthe estimation results we model systemic risk spillover effects across banks byconsidering the marginal effects of the quantile regression procedure. We adopt adropout regularization procedure to remedy the well-known issue of overfittingfor neural networks, and we provide empirical evidence for the favorable out-of-sample performance of a regularized neural network. We then propose threemeasures for systemic risk from our fitted results. We find that systemic riskincreases sharply during the height of the financial crisis in 2008 and againafter a short period of easing in 2011 and 2015. Our approach also allowsidentifying systemically relevant firms during the financial crisis.Keywords:Systemic risk, CoVaR, Quantile regression, Neural networks
SEP
Abstract:The aim of this paper is to prove the phenotypic convergence ofcryptocurrencies, in the sense that individual cryptocurrencies respond tosimilar selection pressures by developing similar characteristics. In order toretrieve the cryptocurrencies phenotype, we treat cryptocurrencies as financialinstruments (genus proximum) and find their specific difference (differentia specifica) by using the daily time series of log-returns. In this sense, a daily timeseries of asset returns (either cryptocurrencies or classical assets) can becharacterized by a multidimensional vector with statistical components likevolatility, skewness, kurtosis, tail probability, quantiles, conditional tailexpectation or fractal dimension. By using dimension reduction techniques(Factor Analysis) and classification models (Binary Logistic Regression,Discriminant Analysis, Support Vector Machines, K-means clustering, VarianceComponents Split methods) for a representative sample of cryptocurrencies,stocks, exchange rates and commodities, we are able to classify cryptocurrenciesas a new asset class with unique features in the tails of the log-returnsdistribution. The main result of our paper is the complete separation of thecryptocurrencies from the other type of assets, by using the Maximum VarianceComponents Split method. More, we observe a divergent evolution of thecryptocurrencies species, compared to the classical assets, mainly due to thetails behaviour of the log-returns distribution. The codes used here areavailable via www.quantlet.de.Keywords:cryptocurrency, genus proximum, differentia specifica, classification,multivariate analysis, factor models, phenotypic convergence, divergentevolution
SEP
Abstract:The paper presents a systematic theory for asymptotic inferences based onautocovariances of stationary processes. We consider nonparametric tests for serial correlations using the maximum and the quadratic deviations of sampleautocovariances. For these cases, with proper centering and rescaling, theasymptotic distributions of the deviations are Gumbel and Gaussian, respectively. To establish such an asymptotic theory, as byproducts, we develop anormal comparison principle and propose a sufficient condition for summabilityof joint cumulants of stationary processes. We adapt a blocks of blocksbootstrapping procedure proposed by Kuensch (1989) and Liu and Singh (1992) tothe maximum deviation based tests to improve the finite-sample performance.Keywords:Autocovariance, blocks of blocks bootstrapping, Box-Pierce test, extreme valuedistribution, moderate deviation, normal comparison, physical dependencemeasure, short range dependence, stationary process, summability of cumulants
SEP
Abstract:The 2017 bubble on the cryptocurrency market recalls our memory in the dot-combubble, during which hard-to-measure fundamentals and investors’ illusion forbrand new technologies led to overvalued prices. Benefiting from the massiveincrease in the volume of messages published on social media and message boards,we examine the impact of investor sentiment, conditional on bubble regimes, oncryptocurrencies aggregate return prediction. Constructing a crypto-specificlexicon and using a local-momentum autoregression model, we find that thesentiment effect is prolonged and sustained during the bubble while it turns outa reversal effect once the bubble collapsed. The out-of-sample analysis alongwith portfolio analysis is conducted in this study. When measuring investorsentiment for a new type of asset such as cryptocurrencies, we highlight thatthe impact of investor sentiment on cryptocurrency returns is conditional onbubble regimes.Keywords:Cryptocurrency; Sentiment; Bubble; Return Predictability
SEP
Abstract:We distill tone from a huge assortment of NASDAQ articles to examine thepredictive power of media-expressed tone in single-stock option markets andequity markets. We find that (1) option markets are impacted by media tone; (2)option variables predict stock returns along with tone; (3) option variablesorthogonalized to public information and tone are more effective predictors ofstock returns; (4) overnight tone appears to be more informative than trading-time tone, possibly due to a different thematic coverage of the trading versusthe overnight archive; (5) tone disagreement commands a strong positive riskpremium above and beyond market volatility.Keywords:option markets, equity markets, stock return predictability, media tone, topicmodel
SEP
Abstract:Increasingly volatile and distributed energy production challenge traditionalmechanisms to manage grid loads and price energy. Local energy markets (LEMs)may be a response to those challenges as they can balance energy production andconsumption locally and may lower energy costs for consumers. Blockchain-basedLEMs provide a decentralized market to local energy consumer and prosumers. Theyimplement a market mechanism in the form of a smart contract without the needfor a central authority coordinating the market. Recently proposed blockchain-based LEMs use auction designs to match future demand and supply. Thus, suchblockchain-based LEMs rely on accurate short-term forecasts of individualhouseholds’ energy consumption and production. Often, such accurate forecastsare simply assumed to be given. The present research tests this assumption.First, by evaluating the forecast accuracy achievable with state-of-the-artenergy forecasting techniques for individual households and, second, byassessing the effect of prediction errors on market outcomes in three differentsupply scenarios. The evaluation shows that, although a LASSO regression modelis capable of achieving reasonably low forecasting errors, the costly settlementof prediction errors can offset and even surpass the savings brought toconsumers by a blockchain-based LEM. This shows, that due to prediction errors,participation in LEMs may be uneconomical for consumers, and thus, has to betaken into consideration for pricing mechanisms in blockchain-based LEMs.Keywords:Blockchain; Local Energy Market; Smart Contract; Machine Learning; Household;Energy Prediction; Prediction Errors; Market Mechanism
SEP
Abstract:We consider a new procedure for detecting structural breaks in mean for high-dimensional time series. We target breaks happening at unknown time points andlocations. In particular, at a fixed time point our method is concerned witheither the biggest break in one location or aggregating simultaneous breaks overmultiple locations. We allow for both big or small sized breaks, so that we ca), stamp the dates and the locations of the breaks, 2), estimate the breaksizes and 3), make inference on the break sizes as well as the break dates. Ourtheoretical setup incorporates both temporal and crosssectional dependence, andis suitable for heavy-tailed innovations. We derive the asymptotic distributionfor the sizes of the breaks by extending the existing powerful theory on locallinear kernel estimation and high dimensional Gaussian approximation to allowfor trend stationary time series with jumps. A robust long-run covariance matrixestimation is proposed, which can be of independent interest. An application ondetecting structural changes of the US unemployment rate is considered toillustrate the usefulness of our method.Keywords:high-dimensional time series, multiple change-points, Gaussian approximation,nonparametric estimation, heavy tailed, long-run covariance matrix
SEP
Abstract:In this paper, we build an overlapping generation model to examine the reasonwhy developed countries with similar background have implemented differentsocial health insurance systems. We propose two hypotheses to explain thisphenomenon: (i) the different participation rates of the poor in the voting;(ii) the distinct attitudes towards the size of the government and the existenceof a compulsory social health insurance system. Agents need to vote for one oftwo policies: Policy I without Social Health Insurance (SHI) but with thesubsidy for the poor, and Policy II with fully covered SHI. By comparing eithertheir current utility or the expected life time utility, households will chooseone policy. We find that under Policy I, the derivative of the changes ofexpected utility with respect to income is not monotonic. This means that boththe poorest and the richest dislike the social health insurance system. With thecalibrated parameters, we solve the benchmark and find that the public’sattitude towards the size of the government and the lower representation of thepoor affect the election result. The changes in the minimum consumption levelunder Policy I affect the voting results most, followed by the attitude. VotingParticipant rate plays the most insignificant role in the voting outcome. Thesensitivity analysis shows that our main findings are robust to the inputparameters.Keywords:Social Health Insurance, Voting
SEP
Abstract:In this paper, we develop an multi period overlapping generation framework toinvestigate agents' consumption and saving decisions, inequality and welfareamong elderly. We assume that agents are heterogeneous in the non-asset incomeand the medical expenditure. In order to explicitly analyze the e ects ofmedical expenditure, we conduct three counterfactual exercises. We successivelyshut down the heterogeneity in labor income, in the level and in the dispersionof medical expenses respectively. By comparing the benchmark with thecounterfactual results, we  nd that in general wealth inequality decreases withage, and income uncertainty contributes the most to wealth inequality. Bothaverage consumption and consumption inequality increase with age. Consumptioninequality largely tracks income inequality. Though uncertainty in medicalexpenditures has little e ect on consumption inequality, a higher level ofmedical expenditures may exacerbate consumption inequality. Meanwhile, theaverage saving of elderly exhibits an inverse-U shape with age. The impacts onaverage saving are similar both in benchmark and in counterfactual exercises.Welfare increases with age.Keywords:Income Inequality, Social Mobility, Price-to-rent ratio
SEP
Abstract:Housing typically takes up a major proportion of households' expenditure, andthus it certainly plays a critical role in shaping the pattern of income in-equality and social mobility. Whether high housing price-to-rent ratio will am-plify inequality and inhibit social class upgrading is still a controversialissue in the existing literature. In this paper, we develop a partialequilibrium life- cycle framework to address these issues. Agents in our economyare divided into two social classes according to the initial human capital levelinherited from their parents. Those who belong to upper class will draw theirinnate abilities from a distribution that  rst order stochastically dominatesthose from lower class. Throughout the entire lifecycle, agents make endogenoushuman capital investment and housing tenure decisions. We calibrate the model tomimic some stylized facts in the the real world counter part. Our simulationresults indicate an inverse-U pattern between housing price-to-rent ratio andmeasures of income inequality, and as well as a U-shape pattern between price-to-rent ratio and social mobility measured by Shorrocks Index. The implicationis that housing tends to amplify the inequality and slow down the socialmobility when houses can only be purchased by a small group of agents in theeconomy. Moreover, our results also suggest that better quality of education asa result of a higher return to human capital investment tends to dampen the roleof housing.Keywords:Income Inequality, Social Mobility, Price-to-rent ratio
SEP
Abstract:Cryptocurrencies are becoming an attractive asset class and are the focus ofrecent quantitative research. The joint dynamics of the cryptocurrency marketyields information on network risk. Utilizing the adaptive LASSO approach, webuild a dynamic network of cryptocurrencies and model the latent communitieswith a dynamic stochastic blockmodel. We develop a dynamic covariate-assistedspectral clustering method to uniformly estimate the latent group membership ofcryptocurrencies consistently. We show that return inter-predictability andcrypto characteristics, including hashing algorithms and proof types, jointlydetermine the crypto market segmentation. Based on this classification result,it is natural to employ eigenvector centrality to identify a cryptocurrency’sidiosyncratic risk. An asset pricing analysis finds that a cross-sectionalportfolio with a higher centrality earns a higher risk premium. Further testsconfirm that centrality serves as a risk factor well and delivers valuableinformation content on cryptocurrency markets.Keywords:Community Detection, Dynamic Stochastic Blockmodel, Spectral Clustering, NodeCovariate, Return Predictability, Portfolio Management
SEP
Abstract:Deep learning has substantially advanced the state-of-the-art in computervision, natural language processing and other  elds. The paper examines thepotential of contemporary recurrent deep learning architectures for  nancialtime series forecasting. Considering the foreign exchange market as testbed, wesystematically compare long short-term memory networks and gated recurrent unitsto traditional recurrent architectures as well as feedforward networks in termsof their directional forecasting accuracy and the profitability of trading modelpredictions. Empirical results indicate the suitability of deep networks forexchange rate forecasting in general but also evidence the diculty ofimplementing and tuning corresponding architectures. Especially with regard totrading pro t, a simpler neural network may perform as well as if not betterthan a more complex deep neural network.Keywords:Deep learning, Financial time series forecasting, Recurrent neural networks,Foreign exchange rates
SEP
Abstract:Risk transmission among financial markets and their participants is time-evolving, especially for the extreme risk scenarios. Possibly sudden timevariation of such risk structures ask for quantitative technology that is ableto cope with such situations. Here we present a novel localized multivariateCAViaR-type model to respond to the challenge of time-varying risk contagion.For this purpose a local adaptive approach determines homogeneous, low riskvariation intervals at each time point. Critical values for this technique arecalculated via multiplier bootstrap, and the statistical properties of this“localized multivariate CAViaR” are derived. A comprehensive simulation studysupports the effectiveness of our approach in detecting structural change inmultivariate CAViaR. Finally, when applying for the US and German financialmarkets, we can trace out the dynamic tail risk spillovers and find that the USmarket appears to play dominate role in risk transmissions, especially involatile market periods.Keywords:conditional quantile autoregression, local parametric approach, change pointdetection, multiplier bootstrap
SEP
Abstract:Understanding the topological structure of real world networks is of hugeinterest in a variety of fields. One of the way to investigate this structure isto find the groups of densely connected nodes called communities. This paperpresents a new non-parametric method of community detection in networks calledAdaptive Weights Community Detection. The idea of the algorithm is to associatea local community for each node. On every iteration the algorithm tests ahypothesis that two nodes are in the same community by comparing their localcommunities. The test rejects the hypothesis if the density of edges betweenthese two local communities is lower than the density inside each one. Adetailed performance analysis of the method shows its dominance over state-of-the-art methods on well known artificial and real world benchmarks.Keywords:Adaptive weights, Gap coefficient, Graph clustering, Nonparametric, Overlappingcommunities
SEP
Abstract:Software-as-a-service applications are experiencing immense growth as theircomparatively low cost makes them an important alternative to traditionalsoftware. Following the initial adoption phase, vendors are now concerned withthe continued usage of their software. To analyze the influence of differentmeasures to improve continued usage over time, a longitudinal study design usingdata from a SaaS vendor was implemented. By employing a linear mixed model, thestudy finds several measures to have a positive effect on a software’s usagepenetration. In addition to these activation measures performed by the SaaSvendor, software as well as client characteristics were likewise examined butdid not display significant estimates. In summary the study contributes novelinsights into the scarcely researched field of influencing factors on SaaS usagecontinuance.Keywords:Linear Mixed Models Software-as-a-Service Usage Continuance
SEP
Abstract:This paper provides a detailed framework for modeling portfolios, achieving thehighest growth rate under subjective risk constraints such as Value at Risk(VaR) in the presence of stable laws. Although the maximization of the expectedlogarithm of wealth induces outperforming any other significantly differentstrategy, the Kelly Criterion implies larger bets than a risk-averse investorwould accept. Restricting the Kelly optimization by spectral risk measures, theauthors provide a generalized mapping for different measures of growth andsecurity. Analyzing over 30 years of S&P 500 returns for different samplingfrequencies, the authors find evidence for leptokurtic behavior for allrespective sampling frequencies. Given that lower sampling frequencies imply asmaller number of data points, this paper argues in favor of α-stable laws andits scaling behavior to model financial market returns for a given horizon in ani.i.d. world. Instead of simulating from the class of elliptically stabledistributions, a nonparametric scaling approximation, based on the data-setitself, is proposed. Our paper also uncovers that including long put optionsinto the portfolio optimization, improves the growth criterion for a givensecurity level, leading to a new Kelly portfolio providing the highest geometricmean.Keywords:growth-optimal, Kelly criterion, protective put, portfolio optimization, stabledistribution, Value at Risk
SEP
Abstract:Weekly, quarterly and yearly risk measures are crucial for risk reportingaccording to Basel III and Solvency II. For the respective data frequencies, theauthors show in a simulation and backtest study that available data series arenot sufficient in order to estimate Value at Risk and Expected Shortfallsufficiently, given confidence levels of 99.9% and 99.99%. Accordingly, thispaper presents a semi-parametric estimation method, rescaling data from high- tolow-frequency which allows to obtain significantly more data points for theestimation of the respective risk measures. The presented methodology in theα-stable framework, which is able to mimic multifractal behavior in assetreturns, provides tail events which never occurred in the original low-frequencydataset.Keywords:high-frequency, multifractal, stable distribution, rescaling, risk management,Value at Risk, quantile distribution
SEP
Abstract:This work aims to investigate the (inter)relations of information arrival, newssentiment, volatilities and jump dynamics of intraday returns. Two parametricGARCH-type jump models which explicitly incorporate both news arrival and newssentiment variables are proposed, among which one assumes news affectingfinancial markets through the jump component while the other postulating theGARCH component channel. In order to give the most-likely format of theinteractions between news arrival and stock market behaviors, these two modelsare compared with several other easier versions of GARCH-type models based onthe calibration results on DJIA 30 stocks. The necessity to include newsprocesses in intraday stock volatility modeling is justified in our specificcalibration samples (2008 and 2013, respectively). While it is not as profitableto model jump process separately as using simpler GARCH process with errordistribution capable to capture fat tail behaviors of financial time series. Inconclusion, our calibration results suggest GARCH-news model with skew-tinnovation distribution as the best candidate for intraday returns of largestocks in US market, which means one can probably avoid the complicatedness ofmodelling jump behavior by using a simplier skew-t error distribution assumptioninstead, but it’s necessary to incorporate news variables.Keywords:information arrival, volatility modeling, jump, sentiment, GARCH
SEP
Abstract:Excessive house price growth was at the heart of the financial crisis i07/08. Since then, many countries have added cooling measures to theirregulatory frameworks. It has been found that these measures can indeed controlprice growth, but no one has examined whether this has adverse consequences forthe housing wealth distribution. We examine this for Singapore, which started i09 to target price growth over ten rounds in total. We find that welfare fromhousing wealth in the last round might not be higher than before 2009. Thisdepends on the deflator used to convert nominal into real prices. Irrespectiveof the deflator, we can reject that welfare increased monotonically over thedifferent rounds.Keywords:house price distribution, stochastic dominance tests
SEP
Abstract:We study investor sentiment on a non-classical asset, cryptocurrencies using a“cryptospecificlexicon” recently proposed in Chen et al. (2018) and statisticallearning methods.We account for context-specific information and word similarityby learning word embeddingsvia neural network-based WorVec model. On top ofpre-trained word vectors, weapply popular machine learning methods such asrecursive neural networks for sentencelevelclassification and sentiment indexconstruction. We perform this analysis on a noveldataset of 1220K messagesrelated to 425 cryptocurrencies posted on a microblogging platformStockTwitsduring the period between March 2013 and May 2018. The constructed sentimentindices are value-relevant in terms of its return and volatility predictabilityfor thecryptocurrency market index.Keywords:sentiment analysis, lexicon, social media, word embedding, deep learning
SEP
Abstract:Second-hand car markets contribute to billions of Euro turnover each year buthardly generate profit for used car dealers. The paper examines the potential ofsophisticated data-driven pricing systems to enhance supplier-side decision-making and escape the zero-profit-trap. Profit maximization requires an accurateunderstanding of demand. The paper identifies factors that characterize consumerdemand and proposes a framework to estimate demand functions using survivalanalysis. Empirical analysis of a large data set of daily used car sales betwee08 to 2012 confirm the merit of the new factors. Observed results also showthe value of survival analysis to explain and predict demand. Random survivalforest emerges as the most suitable vehicle to develop price response functionsas input for a dynamic pricing system.Keywords:Automotive Industry, Price Optimization, Survival Analysis, Dynamic Pricing
SEP
Abstract: A copula model with flexibly specified dependence structure can be useful to capture the complexity and heterogeneity in economic and financial time series. However, there exists little methodological guidance for the specification process using copulas. This paper contributes to fill this gap by considering the recently proposed single-index copulas, for which we propose a simultaneous estimation and variable selection procedure. The proposed method allows to choose the most relevant state variables from a comprehensive set using a penalized estimation, and we derive its large sample properties. Simulation results demonstrate the good performance of the proposed method in selecting the appropriate state variables and estimating the unknown index coefficients and dependence parameters. An application of the new procedure identifies six macroeconomic driving factors for the dependence among U.S. housing markets.  Keywords:Semiparametric Copula, Single-Index Copula, Variable Selection, SCAD  
SEP
Abstract The estimation of a causal parameter in a high-dimensional setting where the functions are potentially complex is a challenging task. Parametric and linear modelling is not sufficient to generate unbiased and consistent estimators. Modern approaches, therefore, use machine learning (ML) algorithms to learn these nuisance functions. However, this leads to new problems like the regularization bias or overfitting that are common when using ML models. This paper considers different novel methods that overcome these problems or at least address them. These methods differ in terms of the target parameter, namely the average treatment effect of the population, group heterogeneity or the conditional average treatment effect for each individual. Each method is first investigated and tested separately and second, they are compared among each other. To do this in a disciplined manner, simulations with synthetic data are used. This ensures that all distributions of the generated treatment effect parameters are known. The findings are that each method has its limits in terms of unbiased estimation, the detection of heterogeneity and also the determination of which covariates are responsible for different causal effects.  Keywords:  causal inference, machine learning, simulation study, sample-splitting double machine learning, sorted group ATE (GATES), causal tree  
SEP
Abstract Uplift modeling combines machine learning and experimental strategies to estimate the differential effect of a treatment on individuals behavior. The paper considers uplift models in the scope of marketing campaign targeting. Literature on uplift modeling strategies is fragmented across academic disciplines and lacks an overarching empirical comparison. Using data from online retailers, we fill this gap and contribute to literature through consolidating prior work on uplift modeling and systematically comparing the predictive performance and utility of available uplift modeling strategies. Our empirical study includes three experiments in which we examine the interaction between an uplift modeling strategy and the underlying machine learning algorithm to implement the strategy, quantify model performance in terms of business value and demonstrate the advantages of uplift models over response models, which are widely used in marketing. The results facilitate making specific recommendations how to deploy uplift models in e-commerce applications.  Keywords:  e-commerce analytics, machine learning, uplift modeling, real-time targeting  
SEP
Abstract In deconvolution in Rd; d  1; with mixing density p(2 P) and kernel h; the mixture density fp(2 Fp) can always be estimated with f^pn; ^pn 2 P; via Minimum Distance Estimation approaches proposed herein, with calculation of f^pn's upper -error rate, an; in probability or in risk; h is either known or unknown, an decreases to zero with n: In applications, an is obtained when P consists either of products of d densities dened on a compact, or  separable densities in R with their dierences changing sign at most J times; J is either known or unknown. When h is known and p is ~q-smooth, vanishing outside a compact in Rd; plug-in upper bounds are then also provided for the -error rate of ^pn and its derivatives, respectively, in probability or in risk; ~q 2 R+; d  1: These -upper bounds depend on h's Fourier transform, ~h(6= 0); and have rates (log a??1 n )?? and a n , respectively, for h super-smooth and smooth;  > 0;  > 0: For the typical an  (log n)  n??; the former (logarithmic) rate bound is optimal for any  > 0 and the latter misses the optimal rate by the factor (log n) when  = :5;  > 0;  > 0: The exponents  and  appear also in optimal rates and lower error and risk bounds in the deconvolution literature.  Keywords:    
SEP
Abstract In linear regression of Y on X(2 Rp) with parameters (2 Rp+1); statistical inference is unreliable when observations are obtained from gross-error model, F;G = (1??)F +G; instead of the assumed probability F;G is gross-error probability, 0 <  < 1: When G is unit mass at (x; y); Residual's In uence Index, RINFIN(x; y; ; ), measures the dierence in small x-perturbations of -residual, r(x; y); for model F and for F;G via r's x-partial derivatives. Asymptotic properties are presented for sample RINFIN that is successful in extracting indications for in uential and bad leverage cases in microarray data and simulated, high dimensional data. Its performance improves as p increases and can also be used in multiple response linear regression. RINFIN's advantage is that, whereas in in uence functions of -regression coecients each x-coordinate and r(x; y) appear in a sum as product with moderate size when (x; y) is bad leverage case and masking makes r(x; y) nearly vanish, RINFIN's x-partial derivatives convert the product in sum allowing for unmasking.  Keywords:  Big Data, Data Science, In fluence Function, Leverage, Masking, Residual's In fluence Index (RINFIN)  
SEP
Abstract High-dimensional, streaming datasets are ubiquitous in modern applications. Examples range from nance and e-commerce to the study of biomedical and neuroimaging data. As a result, many novel algorithms have been proposed to address challenges posed by such datasets. In this work, we focus on the use of - regularized linear models in the context of (possibly non-stationary) streaming data. Recently, it has been noted that the choice of the regularization parameter is fundamental in such models and several methods have been proposed which iteratively tune such a parameter in a time-varying manner, thereby allowing the underlying sparsity of estimated models to vary. Moreover, in many applications, inference on the regularization parameter may itself be of interest, as such a parameter is related to the underlying sparsity of the model. However, in this work, we highlight and provide extensive empirical evidence regarding how various (often unrelated) statistical properties in the data can lead to changes in the regularization parameter. In particular, through various synthetic experiments, we demonstrate that changes in the regularization parameter may be driven by changes in the true underlying sparsity, signal-to-noise ratio or even model misspecication. The purpose of this letter is, therefore, to highlight and catalog various statistical properties which induce changes in the associated regularization parameter. We conclude by presenting two applications: one relating to nancial data and another to neuroimaging data, where the aforementioned discussion is relevant.  Keywords:  Lasso, penalty parameter, stock prices, neuroimaging  
SEP
Abstract The market capitalization of cryptocurrencies has risen rapidly during the last few years. Despite their high volatility, this fact has spurred growing interest in cryptocurrencies as an alternative investment asset for portfolio and risk management. We characterise the effects of adding cryptocurrencies in addition to traditional assets to the set of eligible assets in portfolio management. Out-of-sample performance and diversification benefits are studied for the most popular portfolio-construction rules, including mean-variance optimization, risk-parity, and maximum-diversification strategies, as well as combined strategies.  To account for the frequently low liquidity of cryptocurrency markets we incorporate the LIBRO method, which gives suitable liquidity constraints. Our results show that cryptocurrencies can improve the risk-return profile of portfolios. In particular, cryptocurrencies are more useful for portfolio strategies with higher target returns; they do not play a role in minimum-variance portfolios. However, a maximum-diversification strategy (maximising the Portfolio Diversification Index, PDI) draws appreciably on cryptocurrencies, and spanning tests clearly indicate that cryptocurrency returns are non-redundant additions to the investment universe.  Keywords:  cryptocurrency, CRIX, investments, portfolio management, asset classes, blockchain, Bitcoin, altcoins, DLT  
SEP
Abstract Modeling the joint tails of multiple nancial time series has important im- plications for risk management. Classical models for dependence often encounter a lack of t in the joint tails, calling for additional  exibility. In this paper we introduce a new nonparametric time-varying mixture copula model, in which both weights and depen- dence parameters are deterministic functions of time. We propose penalized trending mixture copula models with group smoothly clipped absolute deviation (SCAD) penal- ty functions to do the estimation and copula selection simultaneously. Monte Carlo simulation results suggest that the shrinkage estimation procedure performs well in s- electing and estimating both constant and trending mixture copula models. Using the proposed model and method, we analyze the evolution of the dependence among four international stock markets, and nd substantial changes in the levels and patterns of the dependence, in particular around crisis periods.  Keywords:  Copula, Time-Varying Copula, Mixture Copula, Copula Selection  
SEP
Abstract In this paper we investigate the statistical properties of cryptocurrencies by using alpha-stable distributions. We also study the benefits of the Metcalfe's law (the value of a network is proportional to the square of the number of connected users of the system) for the evaluation of cryptocurrencies. As the results showed a potential for herding behaviour, we used LPPL models to capture the behaviour of cryptocurrencies exchange rates during an endogenous bubble and to predict the most probable time of the regime switching.  Keywords:  cryptocurrency, Bitcoin, CRIX, Log-Periodic Power Law, Metcalfes law, stable distribution  
SEP
Abstract An extensive empirical literature documents a generally negative relation, named the leverage effect, between asset returns and changes of volatility. It is more challenging to establish such a return-volatility relationship for jumps in high-frequency data. We propose new nonparametric methods to assess and test for a discontinuous leverage effect  i.e. a covariation between contemporaneous jumps in prices and volatility. The methods are robust to market microstructure noise and build on a newly developed price-jump localization and estimation procedure. Our empirical investigation of six years of transaction data from 320 NASDAQ firms displays no unconditional negative covariation between price and volatility cojumps. We show, however, that there is a strong and significant discontinuous leverage effect if one conditions on the sign of price jumps and whether the price jumps are market-wide or idiosyncratic.  Keywords:  High-frequency data, market microstructure, news impact, market-wide jumps, price jump, volatility jump  
SEP
Abstract Open-ended responses are widely used in market research studies. Processing of such responses requires labor-intensive human coding. This paper focuses on unsupervised topic models and tests their ability to automate the analysis of open-ended responses. Since state-ofthe- art topic models struggle with the shortness of open-ended responses, the paper considers three novel short text topic models: Latent Feature Latent Dirichlet Allocation, Biterm Topic Model and Word Network Topic Model. The models are fitted and evaluated on a set of realworld open-ended responses provided by a market research company. Multiple components such as topic coherence and document classification are quantitatively and qualitatively evaluated to appraise whether topic models can replace human coding. The results suggest that topic models are a viable alternative for open-ended response coding. However, their usefulness is limited when a correct one-to-one mapping of responses and topics or the exact topic distribution is needed.  Keywords:  Market research, open-ended responses, text analytics, short text topic models  
SEP
Abstract This paper studies the short-run impacts of temperature on human performance in the computer-mediated environment using server logs of a popular online game in China. Taking advantage of the quasi-experiment of winter central heating policy inChina, we distinguish the impacts of outdoor and indoor temperature and find that low temperatures below 5 ?C decrease game performance significantly. Non-experienced players suffered larger performance drop than experienced ones. Access to central heating attenuates negative impacts of low outdoor temperatures on gamers performance. High temperatures above 21 ?C also lead to drops in game performance.We conclude that expanding the current central heating zone will bring an increase in human performance by approximately 4% in Shanghai and surrounding provinces in the winter. While often perceived as a leisure activity, online gaming requires intense engagement and the deployment of cognitive, social, and motor skills, which are also key skills for productive activities. Our results draw attention to potential damages of extreme temperature on human performance in the modern computer-mediated environment.  Keywords:  Temperature, Human performance, Online game, Heating  
SEP
Abstract In this article, we study a nonparametric approach regarding a general nonlinear reduced form equation to achieve a better approximation of the optimal instrument. Accordingly, we propose the nonparametric additive instrumental variable estimator (NAIVE) with the adaptive group Lasso.We theoretically demonstrate that the proposed estimator is root-n consistent and asymptotically normal. The adaptive group Lasso helps us select the valid instruments while the dimensionality of potential instrumental variables is allowed to be greater than the sample size. In practice, the degree and knots of B-spline series are selected by minimizing the BIC or EBIC criteria for each nonparametric additive component in the reduced form equation. In Monte Carlo simulations, we show that the NAIVE has the same performance as the linear instrumental variable (IV) estimator for the truly linear reduced form equation. On the other hand, the NAIVE performs much better in terms of bias and mean squared errors compared to other alternative estimators under the high-dimensional nonlinear reduced form equation. We further illustrate our method in an empirical study of international trade and growth. Our findings provide  Keywords:  Adaptive group Lasso; Instrumental variables; Nonparametric additive model; Optimal estimator; Variable selection.  
SEP
Abstract The conventional wisdom that housing prices are the present value of future rents ignores the fact that unlike dividends on stocks, rent is not discretionary. Housing price uncertainty can affect household property investments, which in turn affect rent. By extending the theory of investment under uncertainty, we model the renters decision to buy a house and the landlords decision to sell as the exercising of real options of waiting and examine real options effects on rent. Using data from Hong Kong and mainland China, we find a significant effect of housing price on rent and draw important policy implications.  Keywords:    
SEP
Abstract This paper is concerned with selecting important covariates and estimating the index direction simultaneously for high dimensional single-index models. We develop an efficient Threshold Gradient Directed Regularization method via maximizing Distance Covariance (DC-TGDR) between the single index and response variable. Due to the appealing property of distance covariance which can measure nonlinear dependence between random variables, the proposed method avoids estimating the unknown link function of the single index and dramatically reduces computational complexity compared to other methods that use smoothing techniques. It keeps the model-free advantage from the view of sufficient dimension reduction and requires neither predictors nor response variable to be continuous. In addition, the DC-TGDR method encourages a grouping effect. That is, it is capable of choosing highly correlated covariates in or out of the model together. We examine finite-sample performance of the proposed method by Monte Carlo simulations. In a real data analysis, we identify important copy number alterations (CNAs) for gene expression.  Keywords:  Distance covariance, Highdimensional data, Threshold gradient directed regularization, Single-index models, Variable selection.  
SEP
Abstract In this article we develop a tractable procedure for testing strict stationarity in a double autoregressive model and formulate the problem as testing if the top Lyapunov exponent is negative. Without strict stationarity assumption, we construct a consistent estimator of the associated top Lyapunov exponent and employ a random weighting approach for its variance estimation, which in turn are used in a t-type test. We also propose a GLAD estimation for parameters of interest, relaxing key assumptions on the commonly used QMLE. All estimators, except for the intercept, are shown to be consistent and asymptotically normal in both stationary and explosive situations. The nite-sample performance of the proposed procedures is evaluated via Monte Carlo simulation studies and a real dataset of interest rates is analyzed.  Keywords:  DAR model, GLAD estimation, Nonstationarity, Random weighting, Strict stationarity testing.  
SEP
Abstract In this paper, we propose a new class of regime shift models with flexible switching mechanism that relies on a nonparametric probability function of the observed threshold variables. The proposed models generally embrace traditional threshold models with contaminated threshold variables or heterogeneous threshold values, thus gaining more power in handling complicated data structure. We solve the identification issue by imposing either global shape restriction or boundary condition on the nonparametric probability function. We utilize the natural connection between penalized splines and hierarchical Bayes to conduct smoothing. By adopting different priors, our procedure could work well for estimations of smooth curve as well as discontinuous curves with occasionally structural breaks. Bayesian tests for the existence of threshold effects are also conducted based on the posterior samples from Markov chain Monte Carlo (MCMC) methods. Both simulation studies and an empirical application in predicting the U.S. stock market returns demonstrate the validity of our methods.  Keywords:  Threshold Model, Nonparametric, Markov Chain Monte Carlo, Bayesian Inference, Spline.  
SEP
Abstract In this article, we propose a new class of semiparametric instrumental variable models with partially varying coefficients, in which the structural function has a partially linear form and the impact of endogenous structural variables can vary over different levels of some exogenous variables. We propose a three-step estimation procedure to estimate both functional and constant coefficients. The consistency and asymptotic normality of these proposed estimators are established. Moreover, a generalized F-test is developed to test whether the functional coefficients are of particular parametric forms with some underlying economic intuitions, and furthermore, the limiting distribution of the proposed generalized F-test statistic under the null hypothesis is established. Finally, we illustrate the finite sample performance of our approach with simulations and two real data examples in economics.  Keywords:  Endogeneity; Functional coefficients; Generalized F-test; Instrumental variables models; Nonparametric test; Profile least squares  
SEP
Abstract We model the term structure of implied volatility (TSIV) with an adaptive approach to improve predictability, which treats dynamic time series models of globally time- varying but locally constant parameters and uses a data-driven procedure to ?nd the local optimal interval. We choose two speci?cations of the adaptive models: a simple local AR (LAR) model for a univariate implied volatility series and an adaptive dynamic Nelson-Siegel (ADNS) model of three factors, each based on an LAR, to model the cross- section of the TSIV simultaneously with parsimony. Both LAR and ADNS models uniformly outperform more than a dozen alternative models with signi?cance across maturities for 1-20 day forecast horizons. Measured by RMSE and MAE, the forecast errors of the random walk model can be reduced by between 20% and 60% for the 5 to 20 days ahead forecast. In terms of prediction accuracy of future directional changes, the adaptive models achieve an accuracy range of 60%-90%, which strictly dominates the range of 30%-59% of the alternative models.  Keywords:  Term structure of implied volatility, local parametric models, forecasting  
SEP
Abstract A trading rule that draws on the empirical similarity concept is proposed to simulate the technical trading mentality|one that selectively perceives structural resemblances between market scenarios of the present and the past. In more than half of the nineteen futures markets that we test against for protability of this similarity-based trading rule, we nd evidence of predictive ability that is robust to data-snooping and transaction-cost adjust- ments. When aided by an exit strategy that liquidates the trader's positions across some evenly-spaced time points, this rule generates the most robust returns.  Keywords:  empirical similarity; technical trading; futures markets; analogical reasoning  
SEP
Abstract Cryptocurrencies refer to a type of digital cash that use distributed ledger - or blockchain technology - to provide secure transactions. These currencies are generally misunderstood. While initially dismissed as fads or bubbles, many large central banks are considering launching their own version of national cryptocurrencies. In contrast to most data in nancial economics, there is a plethora of detailed (free) data on the history of every transaction for the cryptocurrency complex. Further, there is little empirically-oriented research on this new asset class. This is an extraordinary research opportunity for academia. We provide a starting point by giving an insight into cryptocurrency mechanisms and detailing summary statistics and focusing on potential future research avenues in nancial economics.  Keywords:  Cryptocurrency, Blockchain, Bitcoin, Economic bubbles, Peer-to-Peer, Cryptographic hashing, Consensus, Proof-of-Work, Proof-of-stake, Volatility  
SEP
Abstract News move markets and contains incremental information about stock reactions. Future trading volumes, volatility and returns are a ected by sentiments of texts and opinions expressed in articles. Earlier work of sentiment distillation of stock news suggests that risk prole reactions might differ across sectors. Conventional asset pricing theory recognizes the role of a sector and its risk uniqueness that differs from market or rm specic risk. Our research assesses whether incorporating the sentiment distilled from sector specic news carries information about risk proles. Textual analytics applied to about 600K articles leads us with lexical projection and machine learning to classication of sentiment polarities. The texts are scraped from offcial NASDAQ web pages and with Natural Language Processing (NLP) techniques, such as tokenization, lemmatization, a sector specic sentiment is extracted using a lexical approach and a nancial phrase bank. Predicted sentence-level polarities are aggregated into a bullishness measure on a daily basis and fed into a panel regression analysis with sector indicators. Supervised learning with hinge or logistic loss and regularization yields good prediction results of polarity. Compared with standard lexical projections, the supervised learning approach yields superior predictions of sentiment, leading to highly sector specic sentiment reactions. The Consumer Staples, Health Care and Materials sectors show strong risk prole reactions to negative polarity.  Keywords:  Investor Sentiment, Attention Analysis, Sector-specic Reactions, Volatility, Text Mining, Polarity  
SEP
Abstract In this paper, the complete convergence for maximal weighted sums of extended negatively dependent (END, for short) random variables is investigated. Some sucient conditions for the complete convergence and some applications to a nonparametric model are provided. The results obtained in the paper generalise and improve the corresponding ones of Wang el al. (2014b) and Shen, Xue, and Wang (2017).  Keywords:  Complete convergence; Maximal weighted sums; Extended negatively dependent.  
SEP
Abstract We consider a generalization of Baum-Katz theorem for random vari- ables satisfying some cover conditions. Consequently, we get the result for many dependent structure, such as END, -mixing, -mixing and -mixing, etc.  Keywords:  Complete convergence; Marcinkiewicz-Zygmund type SLLN; Extended negatively dependent; Mixing dependency; Weakly mean bounded.  
SEP
Abstract In this paper, the complete convergence and complete moment convergence for maximal weighted sums of extended negatively dependent random variables are investigated. Some su±cient conditions for the convergence are provided. In addition, the Marcinkiewicz{Zygmund type strong law of large numbers for weighted sums of extended negatively dependent random variables is obtained. The results obtained in the article extend the corresponding ones for independent random variables and some dependent random variables.  Keywords:  Extended negatively dependent, complete convergence, complete moment convergence, maximal weighted sums, strong law of large numbers  
SEP
Abstract In the present paper we propose a new method, the Penalized Adaptive Method (PAM), for a data driven detection of structural changes in sparse linear models. The method is able to allocate the longest homogeneous intervals over the data sample and simultaneously choose the most proper variables with the help of penalized regression models. The method is simple yet  exible and can be safely applied in high-dimensional cases with dierent sources of parameter changes. Comparing with the adaptive method in linear models, its combination with dimension reduction yields a method which properly selects signicant variables and detects structural breaks while steadily reduces the forecast error in high-dimensional data.  Keywords:  SCAD penalty, propagation-separation, adaptive window choice, multiplier bootstrap  
SEP
Abstract Starting from well-known empirical stylised facts of nancial time series, we develop dynamic portfolio protection trading strategies based on econometric methods. As a criterion for riskiness we consider the evolution of the value-at-risk spread from a GARCH model with normal innovations relative to a GARCH model with generalised innovations. These generalised innovations may for example follow a Student t, a generalised hyperbolic (GH), an alpha-stable or a Generalised Pareto (GPD) distribution. Our results indicate that the GPD distribution provides the strongest signals for avoiding tail risks. This is not surprising as the GPD distribution arises as a limit of tail behaviour in extreme value theory and therefore is especially suited to deal with tail risks. Out-of-sample backtests on 11 years of DAX futures data, indicate that the dynamic tail-risk protection strategy eectively reduces the tail risk while outperforming traditional portfolio protection strategies. The results are further validated by calculating the statistical signicance of the results obtained using bootstrap methods. A number of robustness tests including application to other assets further underline the eectiveness of the strategy. Finally, by empirically testing for second order stochastic dominance, we nd that risk averse investors would be willing to pay a positive premium to move from a static buy-and-hold investment in the DAX future to the tail-risk protection strategy.  Keywords:  tail-risk protection, portfolio protection, extreme events, tail distributions  
SEP
Abstract We investigate default probabilities and default correlations of Merton-type credit portfolio models in stress scenarios where a common risk factor is truncated. The analysis is performed in the class of elliptical distributions, a family of light-tailed to heavy-tailed distributions encompassing many distributions commonly found in nancial modelling. It turns out that the asymptotic limit of default probabilities and default correlations depend on the max-domain of the elliptical distribution's mixing variable. In case the mixing variable is regularly varying, default probabilities are strictly smaller than 1 and default correlations are in (0; 1). Both can be expressed in terms of the Student t-distribution function. In the rapidly varying case, default probabilities are 1 and default correlations are 0. We compare our results to the tail dependence function and discuss implications for credit portfolio modelling.   Keywords:  financial risk management, credit portfolio modelling, stress testing, elliptic distribution, max-domain  MSC classification:  60, 91
SEP
Abstract Paralleling regulatory developments, we devise value-at-risk and expected shortfall type risk measures for the potential losses arising from using misspecied models when pricing and hedging contingent claims. Essentially, losses from model risk correspond to losses realized on a perfectly hedged position. Model uncertainty is expressed by a set of pricing models, relative to which potential losses are determined. Using market data, a unied loss distribution is attained by weighing models according to a relative likelihood criterion. Examples demonstrate the magnitude of model risk and corresponding capital buers necessary to suciently protect trading book positions against unexpected losses from model risk.   Keywords:  Model risk, parameter uncertainty, hedge error, value-at-risk, expected shortfall  JEL Clasification: , 
SEP
Abstract We investigate correlations of asset returns in stress scenarios where a common risk factor is truncated. Our analysis is performed in the class of normal variance mixture (NVM) models, which encompasses many distributions commonly used in nancial modelling. For the special cases of jointly normally and t-distributed asset returns we derive closed formulas for the correlation under stress. For the NVM distribution, we calculate the asymptotic limit of the correlation under stress, which depends on whether the variables are in the maximum domain of attraction of the Frechet or Gumbel distribution. It turns out that correlations in heavy-tailed NVM models are less sensitive to stress than in medium- or light-tailed models. Our analysis sheds light on the suitability of this model class to serve as a quantitative framework for stress testing, and as such provides valuable information for risk and capital management in nancial institutions, where NVM models are frequently used for assessing capital adequacy. We also demonstrate how our results can be applied for more prudent stress testing.   Keywords:  Stress testing, risk management, correlation, normal variance mixture distribution, multivariate normal distribution, multivariate t-distribution.
SEP
Abstract In 2012, JPMorgan accumulated a USD 6.2 billion loss on a credit derivatives portfolio, the so-called \London Whale", partly as a consequence of de-correlations of non-perfectly correlated positions that were supposed to hedge each other. Motivated by this case, we devise a factor model for correlations that allows for scenario-based stress-testing of correlations. We derive a number of analytical results related to a portfolio of homogeneous assets. Using the concept of Mahalanobis distance, we show how to identify adverse scenarios of correlation risk. As an example, we apply the factor-model approach to the \London Whale" portfolio and determine the value-at-risk impact from correlation changes. Since our ndings are particularly relevant for large portfolios, where even small correlation changes can have a large impact, a further application would be to stress-test portfolios of central counterparties, which are of systemically relevant size.   Keywords:  Correlation stress testing, scenario selection, market risk, "London Whale"  JEL Classication:  , , , 
SEP
Abstract In a continuous-time setting where a risk-averse agent controls the drift of an output process driven by a Brownian motion, optimal contracts are linear in the terminal output; this result is well-known in a setting with moral hazard and  under stronger assumptions  adverse selection. We show that this result continues to hold when in addition reser- vation utilities are type-dependent. This type of problem occurs in the study of optimal compensation problems involving competing principals.   Keywords:  Principal-agent modelling; contract design; stochastic process; stochastic control
SEP
Abstract In this paper, we study the latent group structure in cryptocurrencies market by forming a dynamic return inferred network with coin attributions. We develop a dynamic covariate-assisted spectral clustering method to detect the communities in dynamic network framework and prove its uniform consistency along the horizons. Applying our new method, we show the return inferred network structure and coin attributions, including algorithm and proof types, jointly determine the market segmentation. Based on the network model, we propose a novel \hard-to-value" measure using the centrality scores. Further analysis reveals that the group with a lower centrality score exhibits stronger short-term return reversals. Cross-sectional return predictability further conrms the economic meanings of our grouping results and reveal important portfolio management implications.   Keywords:  Community Detection, Dynamic Network, Return Predictability, Behavioural Bias, Market Segmentation, Bitcoin
SEP
Abstract IV regression in the context of a re-sampling is considered in the work. Comparatively, the contribution in the development is a structural identication in the IV model. The work also contains a multiplier-bootstrap justication.   Keywords:  Gaussian Process, Kernel methods, Wasserstein Distance 
SEP
Abstract In this work, we propose to define Gaussian Processes indexed by multidimensional distributions. In the framework where the distributions can be modeled as i.i.d realizations of a measure on the set of distributions, we prove that the kernel defined as the quadratic distance between the transportation maps, that transport each distribution to the barycenter of the distributions, provides a valid covariance function. In this framework, we study the asymptotic properties of this process, proving micro ergodicity of the parameters.   Keywords:  Gaussian Process, Kernel methods, Wasserstein Distance  
SEP
Abstract We consider a problem of multiclass classification, where the training sample Sn = {(Xi, Yi)}n i=1 is generated from the model P(Y = m|X = x) = m(x), 1 6 m 6 M, and 1(x), . . . , M(x) are unknown Lip- schitz functions. Given a test point X, our goal is to estimate 1(X), . . . , M(X). An approach based on nonparametric smoothing uses a localization technique, i.e. the weight of observation (Xi, Yi) depends on the distance between Xi and X. However, local estimates strongly depend on localiz- ing scheme. In our solution we fix several schemes , . . . ,WK, compute corresponding local estimates e(1), . . . , e(K) for each of them and apply an aggregation procedure. We propose an algorithm, which constructs a con- vex combination of the estimates e(1), . . . , e(K) such that the aggregated estimate behaves approximately as well as the best one from the collection e(1), . . . , e(K). We also study theoretical properties of the procedure, prove oracle results and establish rates of convergence under mild assumptions.   Keywords:    
SEP
Abstract In the work a characterization of difference of multivariate Gaussian measures is found on the family of centered Eucledian balls. In particular, it helps to derive (xx see paper).   Keywords:  multivariate Gaussian measure, Kolmogorov distance, Gaussian comparison  
SEP
Abstract Let ; : : : ;Xn be i.i.d. sample in Rp with zero mean and the covariance matrix  . The classic principal component analysis esti- mates the projector P J onto the direct sum of some eigenspaces of  by its empirical counterpart bPJ . Recent papers [20, 23] investigate the asymptotic distribution of the Frobenius distance between the projectors k bPJ ??P J  . The problem arises when one tries to build a condence set for the true projector eectively. We consider the problem from Bayesian perspective and derive an approximation for the posterior distribution of the Frobenius distance between projectors. The derived theorems hold true for non-Gaussian data: the only assumption that we impose is the con- centration of the sample covariance b in a vicinity of  . The obtained results are applied to construction of sharp condence sets for the true pro- jector. Numerical simulations illustrate good performance of the proposed procedure even on non-Gaussian data in quite challenging regime.   Keywords:  covariance matrix, spectral projector, principal component analysis, Bernstein { von Mises theorem.  
SEP
